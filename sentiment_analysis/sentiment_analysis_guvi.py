#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().run_cell_magic('writefile', 'sentiment_app.py', 'import pandas as pd\nimport os\n#Dataloader\nclass SentimentDataloader:\n    def __init__(self, filepath=None):\n        self.filepath = filepath\n        self.df = None\n    def load_data(self):\n        try:\n            if self.filepath is None:\n                raise ValueError("No filepath provided to load_data()")\n            if hasattr(self.filepath, "read"):\n                if getattr(self.filepath, "name", "").endswith(".xlsx"):\n                    self.df = pd.read_excel(self.filepath)\n                else:\n                    self.df = pd.read_csv(self.filepath)\n            else:\n                fp = str(self.filepath)\n                if fp.endswith(".xlsx") or fp.endswith(".xls"):\n                    self.df = pd.read_excel(fp)\n                else:\n                    self.df = pd.read_csv(fp)\n            print("Data loaded successfully.")\n        except Exception as e:\n            print(f"Error while loading the file: {e}")\n            self.df = None\n        return self.df\n    def clean_data(self):\n        if self.df is None:\n            raise ValueError("Data not Loaded")\n        if "date" in self.df.columns:\n            self.df["date"] = pd.to_datetime(self.df["date"], errors="coerce")\n        if "verified_purchase" in self.df.columns:\n            try:\n                self.df["verified_purchase"] = self.df["verified_purchase"].map({"Yes": 1, "No": 0}).fillna(0).astype(int)\n            except Exception:\n                self.df["verified_purchase"] = pd.to_numeric(self.df["verified_purchase"], errors="coerce").fillna(0).astype(int)\n        self.df.drop(columns=["username"], inplace=True, errors="ignore")\n        if "review" in self.df.columns:\n            self.df["review"] = self.df["review"].fillna("").astype(str)\n        if "title" in self.df.columns:\n            self.df["title"] = self.df["title"].fillna("").astype(str)\n        if "rating" in self.df.columns:\n            self.df["rating"] = pd.to_numeric(self.df["rating"], errors="coerce")\n        if "helpful_votes" in self.df.columns:\n            self.df["helpful_votes"] = pd.to_numeric(self.df["helpful_votes"], errors="coerce").fillna(0).astype(int)\n        return self.df\n\n#Preprocessing\nimport re\nfrom tqdm import tqdm\nfrom langdetect import detect\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom deep_translator import GoogleTranslator\n\n\nclass ReviewPreprocessor:\n    def __init__(self, df, translator_source="auto", translator_target="en", skip_translation_for="en"):\n        self.df = df.copy()\n        self.translator = GoogleTranslator(source=translator_source, target=translator_target)\n        try:\n            self.stop_words = set(stopwords.words("english"))\n        except Exception:\n            self.stop_words = set()\n        self.lemmatizer = WordNetLemmatizer()\n        self.skip_translation_for = skip_translation_for\n    def detect_lang(self, text):\n        try:\n            if not isinstance(text, str) or text.strip() == "":\n                return "unknown"\n            return detect(text)\n        except Exception:\n            return "unknown"\n    def translate(self, text):\n        try:\n            return self.translator.translate(text)\n        except Exception:\n            return text\n    def clean_text(self, text):\n        if not isinstance(text, str):\n            return ""\n        text = text.lower()\n        text = re.sub(r"[^a-z\\s]", " ", text)\n        text = re.sub(r"\\s+", " ", text).strip()\n        if not text:\n            return ""\n        words = text.split()\n        if self.stop_words:\n            words = [w for w in words if w not in self.stop_words and len(w) > 2]\n        else:\n            words = [w for w in words if len(w) > 2]\n        words = [self.lemmatizer.lemmatize(w) for w in words]\n        return " ".join(words)\n\n    def preprocess_review(self, review_col="review"):\n        if review_col not in self.df.columns:\n            raise ValueError(f"Column \'{review_col}\' not found in dataframe.")\n        tqdm.pandas()\n        self.df["detected_lang"] = self.df[review_col].progress_apply(self.detect_lang)\n        def maybe_translate(row):\n            lang = row["detected_lang"]\n            txt = row[review_col]\n            if lang == self.skip_translation_for or lang == "unknown":\n                return txt\n            return self.translate(txt)\n        self.df["translated_review"] = self.df.progress_apply(maybe_translate, axis=1)\n        self.df["cleaned_review"] = self.df["translated_review"].progress_apply(self.clean_text)\n        print("Translation and cleaning completed successfully.")\n        return self.df\n\n#EDA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nsns.set(style="whitegrid", palette="coolwarm", font_scale=1.05)\n\nclass ReviewEDA:\n    def __init__(self, df):\n        self.df = df.copy()\n        if "rating" in self.df.columns:\n            self.df["rating"] = pd.to_numeric(self.df["rating"], errors="coerce")\n        if "date" in self.df.columns:\n            self.df["date"] = pd.to_datetime(self.df["date"], errors="coerce")\n        if "helpful_votes" in self.df.columns:\n            self.df["helpful_votes"] = pd.to_numeric(self.df["helpful_votes"], errors="coerce").fillna(0).astype(int)\n        if "review" in self.df.columns:\n            self.df["review_length"] = self.df["review"].astype(str).apply(len)\n        print("EDA class initialized successfully.")\n    def plot_rating_distribution(self):\n        fig, ax = plt.subplots(figsize=(7,5))\n        if "rating" not in self.df.columns:\n            ax.text(0.5, 0.5, "No \'rating\' column", ha=\'center\')\n            return fig\n        sns.countplot(x=\'rating\', data=self.df, palette=\'coolwarm\', ax=ax)\n        ax.set_title("Distribution of Review Ratings", fontsize=14)\n        ax.set_xlabel("Rating (1-5)")\n        ax.set_ylabel("Count")\n        plt.tight_layout()\n        return fig\n    def plot_helpful_review_count(self, threshold=10):\n        fig, ax = plt.subplots(figsize=(6,5))\n        if "helpful_votes" not in self.df.columns:\n            ax.text(0.5, 0.5, "No \'helpful_votes\' column", ha=\'center\')\n            return fig\n        self.df[\'helpful_flag\'] = np.where(self.df[\'helpful_votes\'] >= threshold, \'Helpful\', \'Not Helpful\')\n        sns.countplot(x=\'helpful_flag\', data=self.df, ax=ax, palette=\'pastel\')\n        ax.set_title(f"Helpful vs Not Helpful Reviews (Threshold = {threshold})", fontsize=14)\n        ax.set_xlabel("")\n        ax.set_ylabel("Count")\n        plt.tight_layout()\n        return fig\n    def plot_wordcloud_by_sentiment(self):\n        fig, axes = plt.subplots(1, 2, figsize=(14,7))\n        pos_text = " ".join(self.df[self.df.get(\'rating\', 0) >= 4][\'cleaned_review\'].astype(str))\n        neg_text = " ".join(self.df[self.df.get(\'rating\', 0) <= 2][\'cleaned_review\'].astype(str))\n        wc_pos = WordCloud(width=800, height=400, background_color=\'white\').generate(pos_text if pos_text else " ")\n        wc_neg = WordCloud(width=800, height=400, background_color=\'white\').generate(neg_text if neg_text else " ")\n        axes[0].imshow(wc_pos, interpolation=\'bilinear\')\n        axes[0].set_title("Positive Reviews (4‚Äì5)", fontsize=14)\n        axes[0].axis(\'off\')\n        axes[1].imshow(wc_neg, interpolation=\'bilinear\')\n        axes[1].set_title("Negative Reviews (1‚Äì2)", fontsize=14)\n        axes[1].axis(\'off\')\n        plt.tight_layout()\n        return fig\n    def plot_avg_rating_by_version(self, top_n=10):\n        """Plot average rating for each app version if \'version\' column exists."""\n        if \'version\' not in self.df.columns or \'rating\' not in self.df.columns:\n            st.warning("The dataset does not contain \'version\' or \'rating\' columns.")\n            return None\n        version_stats = (\n            self.df.groupby(\'version\')\n            .agg(avg_rating=(\'rating\', \'mean\'), review_count=(\'rating\', \'count\'))\n            .sort_values(\'review_count\', ascending=False)\n            .head(top_n)\n        )\n\n        if version_stats.empty:\n            st.warning("No version data available for plotting.")\n            return None\n        fig, ax = plt.subplots(figsize=(10, 5))\n        sns.barplot(x=version_stats.index, y=version_stats[\'avg_rating\'], palette=\'viridis\', ax=ax)\n        ax.set_title(f"Top {top_n} App Versions by Average Rating")\n        ax.set_xlabel("App Version")\n        ax.set_ylabel("Average Rating")\n        plt.xticks(rotation=45, ha=\'right\', fontsize=9)\n        plt.tight_layout()\n        return fig\n    def plot_avg_rating_over_time(self):\n        fig, ax = plt.subplots(figsize=(10,5))\n        if \'date\' not in self.df.columns:\n            ax.text(0.5, 0.5, "No \'date\' column", ha=\'center\')\n            return fig\n        df_time = self.df.dropna(subset=[\'date\'])\n        if df_time.empty:\n            ax.text(0.5, 0.5, "No valid date data available", ha=\'center\')\n            return fig\n        trend = df_time.groupby(\'date\')[\'rating\'].mean().reset_index()\n        sns.lineplot(x=\'date\', y=\'rating\', data=trend, marker=\'o\', ax=ax)\n        ax.set_title("Average Rating Over Time", fontsize=14)\n        ax.set_xlabel("Date")\n        ax.set_ylabel("Average Rating")\n        plt.tight_layout()\n        return fig\n    def plot_rating_by_location(self, top_n=10):\n        fig, ax = plt.subplots(figsize=(10,5))\n        if \'location\' not in self.df.columns:\n            ax.text(0.5, 0.5, "No \'location\' column", ha=\'center\')\n            return fig\n        loc_df = self.df.groupby(\'location\')[\'rating\'].mean().sort_values(ascending=False).head(top_n)\n        sns.barplot(x=loc_df.values, y=loc_df.index, palette=\'viridis\', ax=ax)\n        ax.set_title(f"Top {top_n} Locations by Average Rating", fontsize=14)\n        ax.set_xlabel("Average Rating")\n        ax.set_ylabel("Location")\n        plt.tight_layout()\n        return fig\n    def plot_platform_comparison(self):\n        fig, ax = plt.subplots(figsize=(8,5))\n        if \'platform\' not in self.df.columns:\n            ax.text(0.5, 0.5, "No \'platform\' column", ha=\'center\')\n            return fig\n        sns.barplot(x=\'platform\', y=\'rating\', data=self.df, estimator=np.mean, ci=None, ax=ax)\n        ax.set_title("Average Rating by Platform", fontsize=14)\n        ax.set_xlabel("Platform")\n        ax.set_ylabel("Average Rating")\n        plt.tight_layout()\n        return fig\n    def plot_verified_user_comparison(self):\n        fig, ax = plt.subplots(figsize=(6,5))\n        if \'verified_purchase\' not in self.df.columns:\n            ax.text(0.5, 0.5, "No \'verified_purchase\' column", ha=\'center\')\n            return fig\n        verified_avg = self.df.groupby(\'verified_purchase\')[\'rating\'].mean().reset_index()\n        verified_avg[\'verified_purchase\'] = verified_avg[\'verified_purchase\'].map({1: \'Verified\', 0: \'Non-Verified\'})\n        sns.barplot(x=\'verified_purchase\', y=\'rating\', data=verified_avg, ax=ax)\n        ax.set_title("Verified vs Non-Verified User Ratings", fontsize=14)\n        ax.set_xlabel("")\n        ax.set_ylabel("Average Rating")\n        plt.tight_layout()\n        return fig\n    def plot_review_length_by_rating(self):\n        fig, ax = plt.subplots(figsize=(8,5))\n        if \'review_length\' not in self.df.columns or \'rating\' not in self.df.columns:\n            ax.text(0.5, 0.5, "Missing \'review_length\' or \'rating\' column", ha=\'center\')\n            return fig\n        sns.boxplot(x=\'rating\', y=\'review_length\', data=self.df, palette=\'coolwarm\', ax=ax)\n        ax.set_title("Review Length by Rating", fontsize=14)\n        ax.set_xlabel("Rating")\n        ax.set_ylabel("Review Length (Characters)")\n        plt.tight_layout()\n        return fig\n    def plot_top_words_in_1star(self):\n        """Show top 20 most common words in 1-star reviews."""\n        if \'review\' not in self.df.columns or \'rating\' not in self.df.columns:\n            st.warning("Required columns missing for analysis.")\n            return None\n        \n        text_1star = " ".join(self.df[self.df[\'rating\'] == 1][\'review\'].astype(str))\n        if not text_1star.strip():\n            st.warning("No 1-star reviews available.")\n            return None\n        words = [w.lower() for w in re.findall(r\'\\b\\w+\\b\', text_1star) if len(w) > 3]\n        common_words = pd.Series(words).value_counts().head(20)\n        fig, ax = plt.subplots(figsize=(8,5))\n        sns.barplot(x=common_words.values, y=common_words.index, palette="Reds_r", ax=ax)\n        ax.set_title("Top 20 Words in 1-Star Reviews", fontsize=14)\n        ax.set_xlabel("Count")\n        ax.set_ylabel("Words")\n        plt.tight_layout()\n        return fig\n    def plot_top_words_in_5star(self):\n        if \'review\' not in self.df.columns or \'rating\' not in self.df.columns:\n            st.warning("Required columns missing for analysis.")\n            return None        \n        text_5star = " ".join(self.df[self.df[\'rating\'] == 5][\'review\'].astype(str))\n        if not text_5star.strip():\n            st.warning("No 5-star reviews available.")\n            return None\n        words = [w.lower() for w in re.findall(r\'\\b\\w+\\b\', text_5star) if len(w) > 3]\n        common_words = pd.Series(words).value_counts().head(20)\n        fig, ax = plt.subplots(figsize=(8,5))\n        sns.barplot(x=common_words.values, y=common_words.index, palette="Greens_r", ax=ax)\n        ax.set_title("Top 20 Words in 5-Star Reviews", fontsize=14)\n        ax.set_xlabel("Count")\n        ax.set_ylabel("Words")\n        plt.tight_layout()\n        return fig\n\n#ModelTraining    \nimport os\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer, f1_score\nfrom xgboost import XGBClassifier, DMatrix, train as xgb_train\nimport xgboost as xgb\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ["PYTHONHASHSEED"] = str(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed(42)\nclass HybridSentimentTrainer:\n    def __init__(\n        self,\n        df,\n        text_col="cleaned_review",\n        label_col="rating",\n        embed_model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",\n        use_gpu=True,\n        seed=42\n    ):\n        self.df = df.copy()\n        self.text_col = text_col\n        self.label_col = label_col\n        self.emb_model_name = embed_model_name\n        self.use_gpu = use_gpu and torch.cuda.is_available()\n        self.device = "cuda" if self.use_gpu else "cpu"\n        self.seed = seed\n        self.label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}\n\n        print(f"‚öôÔ∏è Using device: {self.device.upper()}")\n        self.embedding_model = SentenceTransformer(self.emb_model_name, device=self.device)\n        self.embeddings = None\n        self.X_train = self.X_test = self.y_train = self.y_test = None\n        self.xgb_model = None\n        self.best_params = None\n        set_seed(self.seed)\n        os.makedirs("models", exist_ok=True)\n    def prepare_labels(self):\n        print(" Preparing dataset and mapping labels...")\n        self.df = self.df.dropna(subset=[self.text_col, self.label_col])\n        self.df = self.df[self.df[self.text_col].astype(str).str.strip() != ""].reset_index(drop=True)\n\n        def map_rating(x):\n            if x <= 2:\n                return 0\n            elif x == 3:\n                return 1\n            else:\n                return 2\n        if "sentiment" not in self.df.columns:\n            self.df["sentiment"] = self.df[self.label_col].apply(map_rating)\n\n        self.df["sentiment"] = self.df["sentiment"].astype(int)\n        print(f" Cleaned dataset ‚Äî {len(self.df)} rows | Class counts: {self.df[\'sentiment\'].value_counts().to_dict()}")\n    def augment_data(self, n_aug=1):\n        if n_aug <= 0:\n            print(" Skipping augmentation (n_aug <= 0)")\n            return\n\n        print(f" Performing augmentation: {n_aug} variants per sample...")\n        augmented_texts, augmented_labels = [], []\n        for text, label in tqdm(zip(self.df[self.text_col].astype(str), self.df["sentiment"]), total=len(self.df)):\n            words = text.split()\n            for _ in range(n_aug):\n                if len(words) > 3:\n                    i, j = random.sample(range(len(words)), 2)\n                    w = words.copy()\n                    w[i], w[j] = w[j], w[i]\n                    augmented_texts.append(" ".join(w))\n                    augmented_labels.append(label)\n                else:\n                    augmented_texts.append(text)\n                    augmented_labels.append(label)\n\n        aug_df = pd.DataFrame({self.text_col: augmented_texts, "sentiment": augmented_labels})\n        self.df = pd.concat([self.df, aug_df], ignore_index=True).reset_index(drop=True)\n        print(f" Augmentation complete ‚Äî dataset size: {len(self.df)}")\n    \n    def generate_embeddings(self, batch_size=32):\n        print(f"Generating embeddings on {self.device.upper()}...")\n        texts = self.df[self.text_col].astype(str).tolist()\n        start = time.time()\n        self.embeddings = self.embedding_model.encode(\n            texts,\n            batch_size=batch_size,\n            show_progress_bar=True,\n            convert_to_numpy=True,\n            device=self.device\n        )\n        print(f" Embeddings generated: {self.embeddings.shape} | Time: {time.time() - start:.2f}s")\n\n    def split_data(self, test_size=0.2):\n        X = self.embeddings\n        y = self.df["sentiment"].astype(int).values\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            X, y, test_size=test_size, stratify=y, random_state=self.seed\n        )\n        print(f" Train/Test split done: Train={self.X_train.shape}, Test={self.X_test.shape}")\n\n    def tune_hyperparameters(self, n_iter=15):\n        print(" Starting randomized hyperparameter tuning...")\n        param_distributions = {\n            "n_estimators": np.arange(100, 600, 100),\n            "max_depth": np.arange(3, 10),\n            "learning_rate": np.linspace(0.01, 0.2, 10),\n            "subsample": np.linspace(0.6, 1.0, 5),\n            "colsample_bytree": np.linspace(0.6, 1.0, 5),\n            "min_child_weight": np.arange(1, 8),\n            "gamma": np.linspace(0, 3, 6),\n            "reg_lambda": np.linspace(0.5, 3.0, 6),\n            "reg_alpha": np.linspace(0, 2.0, 5)\n        }\n\n        xgb_model = XGBClassifier(\n            objective="multi:softmax",\n            num_class=3,\n            eval_metric="mlogloss",\n            tree_method="gpu_hist",\n            predictor="gpu_predictor",\n            use_label_encoder=False,\n            seed=self.seed\n        )\n        random_search = RandomizedSearchCV(\n            estimator=xgb_model,\n            param_distributions=param_distributions,\n            n_iter=n_iter,\n            scoring=make_scorer(f1_score, average="weighted"),\n            cv=3,\n            verbose=2,\n            random_state=self.seed,\n            n_jobs=-1\n        )\n        random_search.fit(self.X_train, self.y_train)\n        print(f"Best parameters: {random_search.best_params_}")\n        print(f" Best F1 (weighted): {random_search.best_score_:.4f}")\n\n        self.best_params = random_search.best_params_\n        self.xgb_model = random_search.best_estimator_\n\n    def train_model(self, num_boost_round=150):\n        print(" Training final XGBoost model (GPU + Early Stopping)...")\n        dtrain = DMatrix(self.X_train, label=self.y_train)\n        dtest = DMatrix(self.X_test, label=self.y_test)\n        params = {\n            "objective": "multi:softmax",\n            "num_class": 3,\n            "eval_metric": "mlogloss",\n            "learning_rate": 0.05,\n            "max_depth": 6,\n            "subsample": 0.8,\n            "colsample_bytree": 0.8,\n            "tree_method": "gpu_hist",\n            "predictor": "gpu_predictor",\n            "gpu_id": 0,\n            "seed": self.seed\n        }\n        self.xgb_model = xgb_train(\n            params=params,\n            dtrain=dtrain,\n            num_boost_round=num_boost_round,\n            evals=[(dtrain, "train"), (dtest, "test")],\n            early_stopping_rounds=20,\n            verbose_eval=50\n        )\n        print("Model training complete!")\n\n    def evaluate_model(self):\n        print(" Evaluating model on test data...")\n        if isinstance(self.xgb_model, xgb.Booster):\n            dtest = DMatrix(self.X_test)\n            y_pred = self.xgb_model.predict(dtest).astype(int)\n        else:\n            y_pred = self.xgb_model.predict(self.X_test).astype(int)\n\n        acc = accuracy_score(self.y_test, y_pred)\n        print(f"\\n Accuracy: {acc:.4f}\\n")\n        print("Classification Report:")\n        print(classification_report(self.y_test, y_pred, target_names=list(self.label_map.values())))\n        cm = confusion_matrix(self.y_test, y_pred)\n        plt.figure(figsize=(5, 4))\n        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",\n                    xticklabels=self.label_map.values(),\n                    yticklabels=self.label_map.values())\n        plt.title("Confusion Matrix - Sentiment Classification")\n        plt.xlabel("Predicted")\n        plt.ylabel("True")\n        plt.tight_layout()\n        plt.show()\n\n    def save_model(self, path="models/hybrid_sentiment_gpu_model"):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        if isinstance(self.xgb_model, xgb.Booster):\n            self.xgb_model.save_model(f"{path}.json")\n        else:\n            self.xgb_model.save_model(f"{path}.json")\n\n        np.save(f"{path}_embeddings.npy", self.embeddings)\n        meta = {\n            "text_col": self.text_col,\n            "label_col": self.label_col,\n            "embed_model_name": self.emb_model_name,\n            "label_map": self.label_map\n        }\n        joblib.dump(meta, f"{path}_meta.joblib")\n        print(f" Model and artifacts saved to {path}.*")\n\n    def run_pipeline(self, augment=True, n_aug=1, tune=False, batch_size=32, num_boost_round=150):\n        start_time = time.time()\n        set_seed(self.seed)\n        self.prepare_labels()\n        if augment:\n            self.augment_data(n_aug=n_aug)\n        self.generate_embeddings(batch_size=batch_size)\n        self.split_data()\n\n        if tune:\n            self.tune_hyperparameters()\n        else:\n            self.train_model(num_boost_round=num_boost_round)\n\n        self.evaluate_model()\n        self.save_model()\n        print(f" Pipeline completed in {(time.time() - start_time)/60:.2f} min.")\n\n\n\n# trainer = HybridSentimentTrainer(cleaned_df)\n# trainer.run_pipeline(augment=True, n_aug=1, tune=True, batch_size=32, num_boost_round=150)\n\n#Predictor\n\nimport joblib\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport xgboost as xgb\n\nclass HybridSentimentPredictor:\n    def __init__(self, model_path="models/hybrid_sentiment_gpu_model"):\n        self.model_path = model_path\n        self.meta = joblib.load(f"{model_path}_meta.joblib")\n        self.embedding_model = SentenceTransformer(self.meta["embed_model_name"])\n        self.xgb_model = xgb.Booster()\n        self.xgb_model.load_model(f"{model_path}.json")\n        self.label_map = self.meta.get("label_map", {0: "Negative", 1: "Neutral", 2: "Positive"})\n\n    def predict(self, texts):\n        if isinstance(texts, str):\n            texts = [texts]\n        if not texts:\n            return []\n        embeddings = self.embedding_model.encode(texts, convert_to_numpy=True)\n        dtest = xgb.DMatrix(embeddings)\n        preds = self.xgb_model.predict(dtest).astype(int)\n        return [self.label_map.get(int(p), "Unknown") for p in preds]\n    def predict_new_data(self, new_df):\n        print("üîç Preprocessing new data...")\n        new_texts = new_df[\'review\'].astype(str).tolist()\n\n        new_embeddings = self.embedding_model.encode(new_texts, show_progress_bar=False)\n\n        y_pred = self.xgb_model.predict(new_embeddings)\n\n        label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}\n        predictions = [label_map.get(int(p), "Unknown") for p in y_pred]\n\n        new_df["Predicted_Sentiment"] = predictions\n        return new_df\n#streamlit ui\nimport streamlit as st\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\n\nst.set_page_config(page_title="üí¨ Sentiment Analysis Dashboard", layout="wide")\nst.title("üí¨ Sentiment Analysis Dashboard")\nst.sidebar.header("Options")\nchoice = st.sidebar.radio("Action", ["Upload & EDA", "Predict Sentiment"])\nif choice == "Upload & EDA":\n    uploaded = st.file_uploader("Upload CSV or Excel", type=["csv", "xlsx"])\n    if uploaded is not None:\n        loader = SentimentDataloader(uploaded)\n        df = loader.load_data()\n        if df is None:\n            st.error("Failed to load file.")\n            st.stop()\n        df = loader.clean_data()\n\n        st.write("### Sample Data")\n        st.dataframe(df.head())\n\n        if "review" in df.columns:\n            pre = ReviewPreprocessor(df)\n            cleaned_df = pre.preprocess_review()\n        else:\n            cleaned_df = df\n\n        st.write("### Cleaned Sample")\n        st.dataframe(cleaned_df.head())\n---\n        try:\n            predictor = HybridSentimentPredictor(model_path="models/hybrid_sentiment_gpu_model")\n\n            device = "cuda" if torch.cuda.is_available() else "cpu"\n            if hasattr(predictor, "model"):\n                predictor.model.to(device)\n\n            try:\n                cleaned_df["Predicted Sentiment"] = cleaned_df["cleaned_review"].apply(predictor.predict)\n            except RuntimeError as e:\n                if "CUDA out of memory" in str(e):\n                    st.warning(" GPU memory full. Switching to CPU mode...")\n                    torch.cuda.empty_cache()\n                    predictor.model.to("cpu")\n                    cleaned_df["Predicted Sentiment"] = cleaned_df["cleaned_review"].apply(predictor.predict)\n                else:\n                    raise e\n\n            st.success(" Sentiment prediction completed for uploaded data.")\n            st.write("### Predicted Sentiments")\n            st.dataframe(cleaned_df[["review", "Predicted Sentiment"]].head())\n\n        except Exception as e:\n            st.warning(f" Sentiment prediction skipped: {e}")\n\n        eda = ReviewEDA(cleaned_df)\n        st.sidebar.subheader("Plots")\n        plots = {\n            "Rating Distribution": eda.plot_rating_distribution,\n            "Helpful Reviews": eda.plot_helpful_review_count,\n            "WordCloud by Sentiment": eda.plot_wordcloud_by_sentiment,\n            "Average Rating Over Time": eda.plot_avg_rating_over_time,\n            "Rating by Location": eda.plot_rating_by_location,\n            "Platform Comparison": eda.plot_platform_comparison,\n            "Verified vs Non-Verified": eda.plot_verified_user_comparison,\n            "Review Length by Rating": eda.plot_review_length_by_rating,\n            "Top Words in 1-Star Reviews": eda.plot_top_words_in_1star,\n            "Average Rating by Version": eda.plot_avg_rating_by_version\n        }\n\n        for name, func in plots.items():\n            if st.sidebar.checkbox(name, value=False):\n                st.markdown(f"### {name}")\n                fig = func()\n                st.pyplot(fig)\n                plt.clf()\n\nelif choice == "Predict Sentiment":\n    st.subheader("Predict sentiment with saved model")\n    text = st.text_area("Enter review text")\n\n    if st.button("Predict"):\n        if not text.strip():\n            st.warning("Please enter some text.")\n        else:\n            try:\n                predictor = HybridSentimentPredictor(model_path="models/hybrid_sentiment_gpu_model")\n\n                device = "cuda" if torch.cuda.is_available() else "cpu"\n                if hasattr(predictor, "model"):\n                    predictor.model.to(device)\n\n                try:\n                    res = predictor.predict(text)\n                    st.success(f"Predicted sentiment: **{res[0]}**")\n\n                except RuntimeError as e:\n                    if "CUDA out of memory" in str(e):\n                        torch.cuda.empty_cache()\n                        predictor.model.to("cpu")\n                        res = predictor.predict(text)\n                        st.success(f"Predicted sentiment (CPU): **{res[0]}**")\n                    else:\n                        raise e\n\n            except Exception as e:\n                st.error(f"Prediction failed: {e}")\n')


# In[ ]:









